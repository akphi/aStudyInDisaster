# Model {#model}

```{r echo = FALSE, include = FALSE}
#install.packages("ResourceSelection")
library(tidyverse)
library(gridExtra)
library(oilabs)
library(gsheet)
library(GGally)
library(ResourceSelection)
titanic <- gsheet2tbl('https://docs.google.com/spreadsheets/d/1Cz3wnwugHApFXO0ogMLtBe5CzD_Uu1q8YBPpZPS2R3U/edit?usp=sharing')
```

## Model Obtained Using Backward Selection

The first model we obtained is by backward selection. We eliminated `number_of_siblings_and_spouses` and `fares` before we obtained a model whose p-value of each variable is smaller than 0.05. The model we obtained and its summary are shown below:
```{r, echo=FALSE}
m_best <- glm(has_survived ~ gender + age + number_of_siblings_and_spouses + passenger_class + embarked_from, data = titanic, family = binomial(link = "logit"))
summary(m_best)
```

As can be seen from the summary, each variable has a p-value that is much lower than 0.05 except `emabrked_fromQ`. However, since the p-value of `emabrked_fromS` is low enough, we do not need to eliminate `embarked_from` variable. Also, there is no huge change of coefficients while we eliminate `number_of_siblings_and_spouses` and `fares`, so we do not need to worry about collinearity for these two variables; otherwise, we have to study whether the low p-value is caused by collinearity. The formula of this model is $$ln(\hat{\frac{p}{1-\hat{p}}})= 4.86097 - 2.60736 \times gendermale -2.00712 \times age[19,55] -3.10763 \times age[56, above) \\-1.76429 \times age[6,18] -2.21886\times agemissing -0.35361 \times number\_of\_siblings\_and\_spouses\\ -0.91904\times passenger\_classsecond -1.77251\times passenger\_classthird\\ -0.47350\times embarked\_fromQ -0.67719\times embarked\_fromS$$ Now, we shall evaluate this model using Hosmer-Lemeshow goodness-of-fit test, and the outcome is as shown below:
```{r, echo=FALSE}
hl <- hoslem.test(titanic$has_survived, fitted(m_best), g=8)
hl
```

The p-value is 0.001065, which is much lower than 0.05; this suggests there is strong evidence that this model has a lack of fit. Hence, the model obtained using purely backward selection may not be the best and thus, we should conduct further feature selection steps to improve our model.



## The Final Models

To get our final model, we first checked our suspicion of collinearity mentioned in the Data Exploration section. We are going to examine the correlation of gender with other variables through the plots below:

```{r, echo=FALSE}
gender.1<-as.data.frame(mosaic::tally(age~gender, data=titanic, format="percent"))
gender.2<-as.data.frame(mosaic::tally(passenger_class~gender, data=titanic, format="percent"))
gender.3<-as.data.frame(mosaic::tally(number_of_siblings_and_spouses ~gender, data=titanic, format="percent"))
gender.4<-as.data.frame(mosaic::tally(embarked_from~gender, data=titanic, format="percent"))

p1<-ggplot(gender.1, aes(x=gender, y=Freq, fill=age))+
  geom_bar(stat = "identity")+
  scale_fill_brewer("")+
  ggtitle("Age")+
  xlab("")+
  ylab("Percentage")

p2<-ggplot(gender.2, aes(x=gender, y=Freq, fill=passenger_class))+
  geom_bar(stat = "identity")+
  scale_fill_brewer("")+
  ggtitle("Passenger Class")+
  xlab("")+
  ylab("Percentage")

p3<-ggplot(gender.3, aes(x=gender, y=Freq, fill=number_of_siblings_and_spouses))+
  geom_bar(stat = "identity")+
  scale_fill_brewer("")+
  ggtitle("Number of Siblings and Spouses")+
  xlab("")+
  ylab("Percentage")

p4<-ggplot(gender.4, aes(x=gender, y=Freq, fill=embarked_from))+
  geom_bar(stat = "identity")+
  scale_fill_brewer("", labels=c("Cherbourg", "Queenstown", "Southampton"))+
  ggtitle("Embarked From")+
  xlab("")+
  ylab("Percentage")


grid.arrange(p1,p2,p3,p4)
```

It seems that there is no significant correlation between `age` and `gender`, or `embarked_from` and `gender`. There is, however, a significant difference in the proportion of between the passenger classes for each gender. The number of siblings and spouses also seems to be correlated with gender. Because of this, we considered making two models, one with `gender` and one without `gender`. 

Also, we looked at the correlation of `embarked_from` and `passenger_class`; the plot is shown below:

```{r, echo=FALSE}
pclass<-as.data.frame(mosaic::tally(passenger_class~embarked_from, data=titanic, format="percent"))

p5<-ggplot(pclass, aes(x=embarked_from, y=Freq, fill=passenger_class))+
  geom_bar(stat = "identity")+
  scale_fill_brewer("")+
  ggtitle("Embarked From vs. Passenger Class")+
  xlab("")+
  ylab("Percentage")+
  scale_x_discrete(labels=c("Cherbourg", "Queenstown", "Southampton"))
p5
```

As can be seen from this graph, there is a very significant correlation between `passenger_class` and `embarked_from`. For example, more than half of the passengers from Cherbourg are first class passengers, and only a few from Queenstown are first class. This suggests that we should be very careful when include both `passenger_class` and `embarked_from` in the same model. 

Next, we created the following models. The one without `gender` takes `passenger_class`, `age` and `number_of_siblings_and_spouses` into account. We tried several models, and despite the fact that we will get a better p-value from Hosmer-Lemeshow goodness-of-fit test by including `embarked_from`, we still think it might be problematic because of its strong association with `passenger_class`. The summary of the best model (without `gender`) found and the result from the Hosmer-Lemeshow goodness-of-fit test is as shown:

```{r, echo=FALSE}
m_best_8 <- glm(has_survived ~ passenger_class + age + number_of_siblings_and_spouses, data = titanic, family = binomial(link = "logit"))
summary(m_best_8)
```

```{r, echo=FALSE}
hl <- hoslem.test(titanic$has_survived, fitted(m_best_8), g=8)
hl
```

As we mentioned before, this p-value is not the best that could be achieved, but it is still good enough given that it is greater than 0.05. The equation of this regression model is: $$ln(\hat{\frac{p}{1-\hat{p}}})= 2.43417 -1.24306 \times age[6,18] -1.78069 \times age[19,55] -2.80888 \times age[56, above)\\ -1.93037\times agemissing -0.15772 \times number\_of\_siblings\_and\_spouses\\ -0.97894\times passenger\_classsecond -1.78063\times passenger\_classthird$$ The negative coefficients of all variables suggest that the reference group is the group with highest survival rate. That is the group of children (age smaller than 5) in first class without any siblings and spouses are the most likely to survive; and the natural log of odds is expected to be 2.43. This means that their chance of survival is approximately `r 1/(1+exp(-2.43))`. There are many other interesting interpretation that can be extracted from this model. For example, it suggests that first class passengers are expected to have a 0.98 higher natural log of odds than second class passengers, all else held constant. Also, we can expect a decrease of 0.16 in natural log of odds for every one more increase in number of siblings and spouses, all else held constant.

This model seems good as it matches with our instinct that children and first class passengers will have better chance of survival in a shipwreck. This could be due to their proximity to the rescue site, or perhaps, first class passengers were more likely to know how to swim given their background. However, since `gender` is such an important variable, we would also like to examine a model with `gender` as one of the explanatory variables. 

We made a set of models and compared them according to their meanings and their results from Hosmer-Lemeshow goodness-of-fit test. It turns out that all the model with `gender` variable fails the Hosmer-Lemeshow goodness-of-fit test since they all have really really low p-value, but arguably, we could conclude that the best model (with `gender`) is the one that takes `gender`, `age`, `passenger_class`, and `number_of_siblings_and_spouses` because it contains the most crucial information of a passenger, and its AIC value is the lowest among all the models including `gender` as an explanatory variable. The summary of this model is as shown below:

```{r, echo=FALSE}
m_best_12 <- glm(has_survived ~ gender + age + number_of_siblings_and_spouses + passenger_class, data = titanic, family = binomial(link = "logit"))
summary(m_best_12)
hl <- hoslem.test(titanic$has_survived, fitted(m_best_12), g=8)
data.frame(as.data.frame.matrix(hl$observed), as.data.frame.matrix(hl$expected))
hl
```

As we can see from the summary, the equation of this regression model is $$ln(\hat{\frac{p}{1-\hat{p}}})= 4.46068 -2.61030 \times gendermale -2.01284 \times age[19,55] -3.08942 \times age[56, above) \\-1.70756 \times age[6,18] -2.15090\times agemissing -0.37239 \times number\_of\_siblings\_and\_spouses\\ -1.11954\times passenger\_classsecond -1.92213\times passenger\_classthird$$ Since all the coefficients are negative, the reference group is expected to be the group of people with highest chance of survival, similar to the previous model. That is, little girls (age under 5) in first class without siblings and spouses are ones with highest survival chance; and the natural log of odds is expected to be 4.46, which means that their chance of survival is approximately `r 1/(1+exp(-4.46))`. Also, the coefficients suggest how each variable affect the chance of survival. For example, males are expected to have a 2.61 lower natural log of odds than females, all else held constant. We can also see that having one more siblings or spouses is associated with 0.37 decrease in natural log of odds, all else held constant.

As for the best model between these two, we chose the latter one, i.e. the model with `gender` variable. We find it interesting to look at the correlation between `gender` and `has_survived`.

```{r, echo=FALSE}
gender.5<-as.data.frame(mosaic::tally(has_survived~gender, data=titanic, format="percent"))
p6<-ggplot(gender.5, aes(x=gender, y=Freq, fill=has_survived))+
  geom_bar(stat = "identity")+
  scale_fill_brewer("", labels=c("not_survived", "survived"))+
  #ggtitle("Age")+
  xlab("")+
  ylab("Percentage")
p6
```

From the graph, we can see that a majority of female survived while much lower percentage of male survived, so there is indeed, a very strong correlation between `gender` and `has_survived`. Thus, we think it makes more sense to have `gender` as one of the explanatory variables in the final model, even if including it causes a decrease in the p-value of Hosmer-Lemeshow goodness-of-fit test result. Also, the AIC decreases significantly after we added `gender` in the model. This suggests that including `gender` will make our model a better fit even if we take penalty of adding new variables. Hence, we conclude our best model as:

$$ln(\hat{\frac{p}{1-\hat{p}}})= 4.46068 -2.61030 \times gendermale -2.01284 \times age[19,55] -3.08942 \times age[56, above) \\-1.70756 \times age[6,18] -2.15090\times agemissing -0.37239 \times number\_of\_siblings\_and\_spouses\\ -1.11954\times passenger\_classsecond -1.92213\times passenger\_classthird$$