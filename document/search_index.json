[
["index.html", "A Study in Disaster Abstract", " A Study in Disaster mauveSushi 2017-12-12 Abstract This research aims to construct a logistic regression model to predict the survival outcome of passenger on the Titanic based on their various demographic and socioeconomic factors, such as age, gender, number of spouses and siblings, passenger class, etc. The findings of this research can be generalized to most shipwreck where the ship is considered big (having capacity of at least 1000 passengers) and the route is through Atlantic Ocean in the early 20th century. We used logistic regression in tandem with backward feature selection to obtain several prediction models. We evaluate each obtained model using the Hosmer-Lemeshow goodness-of-fit test and then conduct a more vigorous feature selection process to come up with the best models. Despite the different approaches, all of the obtained models suggest a common trend that children, woman and first class passengers are among the one with highest chance of survival. "],
["introduction.html", "Introduction", " Introduction On that fateful night April 15, 1912, the great Titanic carrying 2,200 people struck an iceberg and sank, brought down with it 1,500 poor souls. Such tragedy never fails to leave all of us unsettled, but as statisticians, we are naturally drawn to ask ourselves the question: “How do demographic and socioeconomic factors affect chance of survival in disaster?” Perhaps, based on just the titanic data set, such general question cannot be answered, since our findings are probably just generelizable to a specific type of disaster, which is shipwreck and to a time-frame limited to the early 20th century. Also, we have to consider the fact that Titanic is not just another ship… It is (even till today) one of the most luxurious ship ever been built (see figure below - the ship even had swimming pool and tennis court at the lowest level!). As such, we think the findings we obtain using this data set can only be generalized to big ship, i.e. ships whose capacity is at least 1000, crossing the Pacific ocean. Note that this number 1000 seems rather arbitrary, but we found that nowadays, on average, cruise ships with tonnage of around 40,000 usually have capacity of 1,000 passengers1 and most of the bigger ships in early 20th century were around 40,000 or more in tonnage2. For this reason, we formulate our research question as follows: “How do various demographic and socioeconomic factors affect chance of survival in shipwreck where the ship’s capacity is at least 1000 passegners and the route is through the Atlantic Ocean in the early 20th century?” In simpler words, we will attempt to create a model to be able to predict the survival outcome of a passenger, given his/her relevant background information. We constructed several different models with 2 major approaches, one using backward selection and one using a more rigorous feature selection process. Despite this, the models we obtain show a common trend that female, little kids and first-class passengers are more likely to survive the shipwreck. Following this, we will step by step present our data preparation and modelling process, as well as our findings. Figure .: Cross Section of the Titanic "],
["data-explore.html", "Section 1 Data Exploration 1.1 Summary 1.2 Variable Selection 1.3 Data Preparation 1.4 Final Data Set Summary 1.5 Correlation Investigation", " Section 1 Data Exploration 1.1 Summary The data set contains information about 1306 passengers on the Titanic. Recall that there were around 2,200 people on the ship so with this sample size, clearly the data set is a good representation of the population on the Titanic. There are 11 variables in the original data set, listed as follows: Name Type Unit Meaning Value/Range Remark survival nominal previous name of county 0 = No, 1 = Yes name nominal passenger’s name pclass ordinal ticket class 1 = 1st, 2 = 2nd, 3 = 3rd We can use this as a proxy for socio-economic status (SES) sex nominal passenger’s gender m = male, f = female age numerical year passenger’s age [0,80] sibsp numerical person number of siblings/spouses aboard the Titanic [0,8] siblings = brother, sister, stepbrother, stepsister; spouses = husband, wife (mistresses and fiances were ignored) parch numerical person number of parents/children aboard the Titanic [0,6] parents = mother, father; children = son, daughter, stepson and stepdaughter (some children travelled only with nanny, therefore parch = 0 for them) ticket nominal ticket number fare numerical pound ticket fare [0, 93.5] cabin nominal cabin number embarked nominal port of embarkation C = Cherbourg, Q = Queenstown, S = Southampton 1.2 Variable Selection Our response variable is the nominal variable survival (or boolean as its outcome is 0 or 1). As for explanatory variables, we have to make some pre-selection beforehand. We decide to take out several variables which we consider only as identifier variables, such as name and ticket number. Also we did not include the cabin number since this information is missing for too many passengers. The only value it brings to the model is when we can actually compute the shortest distance from each cabin to the rescue area. However, again as mentioned, there are a lot of missing data and also, if distance is our only concern, then passenger class is quite sufficient for this purpose (refer to the cross-section plot of the Titanic in previous section). 1.3 Data Preparation We notice that age is missing for many passenger, but age is such a crucial variable that we cannot simply take away. As such, we discretize age into several bins: missing, 0-5, 6-18, 19-55, and 56 and above. Also, we rename the variables for readability. So the final model we obtained has the following variables: Name Type Unit Value/Range has_survived nominal 0 = No, 1 = Yes passenger_class ordinal 1 = 1st, 2 = 2nd, 3 = 3rd gender nominal male, female age nominal year [0,5], [6,18], [19,55], [56 and above], missing number_of_siblings_and_spouses numerical person [0,8] number_of_parents_and_children numerical person [0,6] fare numerical pound [0, 93.5] embarked_from nominal C = Cherbourg, Q = Queenstown, S = Southampton 1.4 Final Data Set Summary Following is the summary of the final data set: ## has_survived ## 0 1 ## 808 498 ## gender ## female male ## 464 842 ## age ## [0,5] [19,55] [56, above) [6,18] missing ## 56 790 57 140 263 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.0000 0.0000 0.3859 0.0000 9.0000 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0 0.0 0.0 0.5 1.0 8.0 ## passenger_class ## first second third ## 321 277 708 ## embarked_from ## C Q S ## 270 123 913 1.5 Correlation Investigation Following, we create a plot matrix to investigate potential collinearity between variables in our data set. As seen above, it seems that there might be some colinearity between gender and passenger_class as female and male groups have very different first class to third class ratio. Also, notice that passenger_class and embarked_from seem to have some strong association, for example, there seems to be more third class passenger embarking from Southampton than that of other embarkation ports. As for now, we will take note of these observations; we will use them later for a more vigorous feature selection process as we attempt to improve our model. "],
["method.html", "Section 2 Methodology 2.1 Backward Selection 2.2 Logistic Regression 2.3 Homser-Lemeshow Goodness-Of-Fit Test 2.4 Akaike Information Criterion (AIC)", " Section 2 Methodology 2.1 Backward Selection For backward selection, we start out by including all of the variables we have as explanatory variables and construct a regression model from that. We then take out one of the insignificant predictor (predictor whose p-value is greater than the significance level of 0.05), and then construct another model using the remaining variables. Repeat the process until we obtain a model with all significant predictors. 2.2 Logistic Regression In our project, since our response variable is a binary variable, we are going to use logistic regression for the model. Logistic regression is a type of generalized linear model. Its equation is in the form of \\(ln(\\frac{p}{1-p})=\\ln(odds)=\\alpha_0+\\alpha_1x_1+\\alpha_2x_2+\\dots +\\alpha_nx_n\\), where \\(p\\), in this context, is the probability of survival–1 means the passenger survived and 0 otherwise, and \\(x_1, x_2, \\dots, x_n\\) are explanatory variables. The right hand side of this equation is the same as in a linear model, hence it is called generalized linear model. The left-hand side is the natural log of odds, where odds is a representation of probability, and it equals to \\(\\frac{p}{1-p}\\). Once we fit a model, we can predict the probability of success \\(p\\) by \\(p = \\frac{1}{1+e^{-RHS}}\\), where \\(RHS=\\alpha_0+\\alpha_1x_1+\\alpha_2x_2+\\dots +\\alpha_nx_n\\). To perform logistic regression, we can use glm() function, which stands for generalized linear model, in R, and specify the parameter family = binomial(link = &quot;logit&quot;). 2.3 Homser-Lemeshow Goodness-Of-Fit Test Since we decided to use logistic regression for our model, the assumptions we need to verify are not the same as in a linear regression model. For example, we cannot plot a residual plot to see whether the residuals have a pattern because however good or bad a model is, the residuals will always show a pattern, as they follow 2 curves: positive residuals follow the curve \\(1 - \\frac{1}{1+e^{-predicted}}\\) and the negative residuals follow the curve \\(0 - \\frac{1}{1+e^{-predicted}}\\) as seen below: To examine how good a logistic regression model is, we can use Homser-Lemeshow goodness-of-fit test. The idea of this test is to divide the sample into several groups according to their predicted values, and compare the expected proportion of success to the observed proportion of success in each group to see whether there is a significant difference between the expected and the observed proportion. The null hypothesis of this test is that there is no difference between the expected and the observed proportion of success. In other words, if the p-value of this test is too low, it means that we have strong evidence that the fit is not good enough. However, there are some restrictions for this test as well. One of which is that the choice of number of groups may affect the p-value significantly, and there is no clear rule for how to choose the most proper number of groups. Also, it does not take overfitting into consideration. Because of these problems, we will only use Homser-Lemeshow goodness-of-fit test to give us a rough sense of how good a model is, but not necessarily to reject that model altogether. To perform Homser-Lemeshow goodness-of-fit test, we can use hoslem.test() function in ResourceSelection package in R. 2.4 Akaike Information Criterion (AIC) We will also use Akaike information criterion to evaluate our model. This criterion is a method to compare several regression models together. It is a function of the goodness-of-fit of a model and the number of parameters it takes. It gives reward to better goodness-of-fit and gives penalty to increasing number of explanatory variables The advantage of this criterion is that it will give penalty to over-fitting. However, the limitation of this method is that there is no natural scale to compare with, so it is possible that all of our models have poor fit, and we just choose the relatively best one among these. This is very convenient in R. It is included in the summary() function and we can also call it using AIC() function. "],
["model.html", "Section 3 Model 3.1 Model Obtained Using Backward Selection 3.2 The Final Models", " Section 3 Model 3.1 Model Obtained Using Backward Selection The first model we obtained is by backward selection. We eliminated number_of_siblings_and_spouses and fares before we obtained a model whose p-value of each variable is smaller than 0.05. The model we obtained and its summary are shown below: ## ## Call: ## glm(formula = has_survived ~ gender + age + number_of_siblings_and_spouses + ## passenger_class + embarked_from, family = binomial(link = &quot;logit&quot;), ## data = titanic) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.7755 -0.6791 -0.4560 0.7074 2.5697 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.86097 0.47163 10.307 &lt; 2e-16 *** ## gendermale -2.60736 0.15798 -16.505 &lt; 2e-16 *** ## age[19,55] -2.00712 0.38995 -5.147 2.65e-07 *** ## age[56, above) -3.10763 0.53229 -5.838 5.27e-09 *** ## age[6,18] -1.76429 0.43159 -4.088 4.35e-05 *** ## agemissing -2.21886 0.42028 -5.280 1.30e-07 *** ## number_of_siblings_and_spouses -0.35361 0.09297 -3.803 0.000143 *** ## passenger_classsecond -0.91904 0.21489 -4.277 1.90e-05 *** ## passenger_classthird -1.77251 0.19438 -9.119 &lt; 2e-16 *** ## embarked_fromQ -0.47350 0.30504 -1.552 0.120601 ## embarked_fromS -0.67719 0.18791 -3.604 0.000314 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1736.2 on 1305 degrees of freedom ## Residual deviance: 1191.1 on 1295 degrees of freedom ## AIC: 1213.1 ## ## Number of Fisher Scoring iterations: 5 As can be seen from the summary, each variable has a p-value that is much lower than 0.05 except emabrked_fromQ. However, since the p-value of emabrked_fromS is low enough, we do not need to eliminate embarked_from variable. Also, there is no huge change of coefficients while we eliminate number_of_siblings_and_spouses and fares, so we do not need to worry about collinearity for these two variables. (Otherwise, we have to study whether the low p-value is caused by collinearity). The formula of this model is \\[ln(\\hat{\\frac{p}{1-\\hat{p}}})= 4.86097 - 2.60736 \\times gendermale -2.00712 \\times age[19,55] -3.10763 \\times age[56, above) \\\\-1.76429 \\times age[6,18] -2.21886\\times agemissing -0.35361 \\times number\\_of\\_siblings\\_and\\_spouses\\\\ -0.91904\\times passenger\\_classsecond -1.77251\\times passenger\\_classthird\\\\ -0.47350\\times embarked\\_fromQ -0.67719\\times embarked\\_fromS\\] Now, we shall evaluate this model using Hosmer-Lemeshow goodness-of-fit test, and the outcome is as shown below: ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: titanic$has_survived, fitted(m_best) ## X-squared = 22.308, df = 6, p-value = 0.001065 The p-value is 0.001065, which is much lower than 0.05, and this suggests that this model is very likely to have a lack of fit. Hence, backward selection may not be the best way to fit this model, and we should find a better alternative of this model. 3.2 The Final Models To get our final model, we first checked the skeptical correlations we mentioned in the Data Exploration section. First, we are going to check the correlation of gender with other variables, and we can use the plots below to see it: It seems that there is not a significant correlation between age and gender, or embarked_from and gender. There is a significant difference in the proportion of each passenger classes in each gender. The number of siblings and spouses also seems to be correlated with gender. Because of this, we considered making two models, one with gender and one without gender. Also, we looked at the correlation of embarked_from and passenger_class, and the plot is as shown below: As can be seen from this graph, there is a very significant correlation between passenger_class and embarked_from. For example, more than half of the passengers from Cherbourg are first class passengers, and only a few from Queenstown are first class. This suggests that we should be very careful when include both passenger_class and embarked_from in the same model. Then, we created the following models. The one without gender takes passenger_class, age and number_of_siblings_and_spouses into account. We tried several models, and although we will get a better p-value from Hosmer-Lemeshow goodness-of-fit test by adding embarked_from, we still think it may be problematic because of its correlation with passenger_class. The summary of the model and the result from Hosmer-Lemeshow goodness-of-fit test is as shown: ## ## Call: ## glm(formula = has_survived ~ passenger_class + age + number_of_siblings_and_spouses, ## family = binomial(link = &quot;logit&quot;), data = titanic) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1791 -0.8806 -0.7015 0.9754 2.1284 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.43417 0.35008 6.953 3.57e-12 *** ## passenger_classsecond -0.97894 0.17462 -5.606 2.07e-08 *** ## passenger_classthird -1.78063 0.15635 -11.389 &lt; 2e-16 *** ## age[19,55] -1.78069 0.32160 -5.537 3.08e-08 *** ## age[56, above) -2.80888 0.44150 -6.362 1.99e-10 *** ## age[6,18] -1.24306 0.34932 -3.558 0.000373 *** ## agemissing -1.93037 0.34351 -5.620 1.91e-08 *** ## number_of_siblings_and_spouses -0.15772 0.07244 -2.177 0.029468 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1736.2 on 1305 degrees of freedom ## Residual deviance: 1557.0 on 1298 degrees of freedom ## AIC: 1573 ## ## Number of Fisher Scoring iterations: 4 ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: titanic$has_survived, fitted(m_best_8) ## X-squared = 9.1521, df = 6, p-value = 0.1652 As we mentioned before, this p-value is not the best, but this is good enough given that it is larger than 0.05. The equation of this regression model is \\[ln(\\hat{\\frac{p}{1-\\hat{p}}})= 2.43417 -1.24306 \\times age[6,18] -1.78069 \\times age[19,55] -2.80888 \\times age[56, above)\\\\ -1.93037\\times agemissing -0.15772 \\times number\\_of\\_siblings\\_and\\_spouses\\\\ -0.97894\\times passenger\\_classsecond -1.78063\\times passenger\\_classthird\\] The negative coefficients of all variables suggest that the reference group is the luckiest group. That is, children (age smaller than 5) in first class without any siblings and spouses are the most likely to survive, and their natural log of odds is expected to be 2.43. This means that their chance of survival is approximately 0.9190865. This model also suggests the influence of any variable. For example, this model suggests that people in the first class is expected to have a 0.98 higher natural log of odds than people in the second class holding other variables constant. Also, we can expect a decrease of 0.16 in natural log of odds for one more increase in number of siblings and spouses. This model seems good and it matches with our instinct that children and passengers in the first class will have better chance of survival in a shipwreck. However, since gender is such an important variable, we would like to examine a model with gender as an explanatory variable. We made a set of models and compared them according to their meanings and their results from Hosmer-Lemeshow goodness-of-fit test. It turns out that all the model with gender variable fails the Hosmer-Lemeshow goodness-of-fit test, but arguably, we find the best model which takes gender, age, passenger_class, and number_of_siblings_and_spouses because it contains the most information, and the AIC is the lowest among all the models with gender variable. The summary of this model is as shown below: ## ## Call: ## glm(formula = has_survived ~ gender + age + number_of_siblings_and_spouses + ## passenger_class, family = binomial(link = &quot;logit&quot;), data = titanic) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.8653 -0.6894 -0.4535 0.6856 2.5462 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.46068 0.44952 9.923 &lt; 2e-16 *** ## gendermale -2.61030 0.15445 -16.901 &lt; 2e-16 *** ## age[19,55] -2.01284 0.38663 -5.206 1.93e-07 *** ## age[56, above) -3.08942 0.52946 -5.835 5.38e-09 *** ## age[6,18] -1.70756 0.42644 -4.004 6.22e-05 *** ## agemissing -2.15090 0.41233 -5.217 1.82e-07 *** ## number_of_siblings_and_spouses -0.37239 0.09176 -4.058 4.95e-05 *** ## passenger_classsecond -1.11954 0.20723 -5.402 6.57e-08 *** ## passenger_classthird -1.92213 0.18536 -10.369 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1736.2 on 1305 degrees of freedom ## Residual deviance: 1204.1 on 1297 degrees of freedom ## AIC: 1222.1 ## ## Number of Fisher Scoring iterations: 5 ## y0 y1 yhat0 yhat1 ## [0.00548,0.0977] 195 22 199.42682 17.573180 ## (0.0977,0.111] 205 42 219.77092 27.229080 ## (0.111,0.144] 26 6 27.56544 4.434556 ## (0.144,0.225] 148 18 132.71261 33.287390 ## (0.225,0.459] 122 63 109.91946 75.080540 ## (0.459,0.628] 81 87 70.01437 97.985627 ## (0.628,0.798] 21 108 32.54474 96.455263 ## (0.798,0.984] 10 152 16.04564 145.954365 As we can see from the summary, the equation of this regression model is \\[ln(\\hat{\\frac{p}{1-\\hat{p}}})= 4.46068 -2.61030 \\times gendermale -2.01284 \\times age[19,55] -3.08942 \\times age[56, above) \\\\-1.70756 \\times age[6,18] -2.15090\\times agemissing -0.37239 \\times number\\_of\\_siblings\\_and\\_spouses\\\\ -1.11954\\times passenger\\_classsecond -1.92213\\times passenger\\_classthird\\] Since all the coefficients are negative, the reference group is expected to be the luckiest people as is in the previous model. That is, little girls (age under 5) in first class without siblings and spouses are the most likely to survive, and their natrual log of odds is expected to be 4.46, which means that their chance of survival is approximately 0.9885698. Also, the coefficients suggest how each variable affect the chance of survival. For example, males are expected to have a 2.61 lower natural log of odds than females holding other variables constant. We can also see that one more siblings or spouses is associated with 0.37 decrease natural log of odds. As for the best model of these two, we chose the latter one, the one with gender variable. First, we can look at the correlation between gender and has_survived. The plot is as shown below: From the graph, we can see that a majority of female survived while only a few male survived, so there is a very strong correlation between gender and has_survived. Thus, we think it makes more sense to contain gender variable, even if adding it will decrease the p-value of Hosmer-Lemeshow goodness-of-fit test. Also, the AIC decreases significantly after we added gender in the model. This suggests that adding gender will make our model a better fit even if we take penalty of adding new variables. Hence, the model \\[ln(\\hat{\\frac{p}{1-\\hat{p}}})= 4.46068 -2.61030 \\times gendermale -2.01284 \\times age[19,55] -3.08942 \\times age[56, above) \\\\-1.70756 \\times age[6,18] -2.15090\\times agemissing -0.37239 \\times number\\_of\\_siblings\\_and\\_spouses\\\\ -1.11954\\times passenger\\_classsecond -1.92213\\times passenger\\_classthird\\] is our best model. "],
["conclusion.html", "Section 4 Conclusion 4.1 Findings 4.2 Further Study", " Section 4 Conclusion 4.1 Findings In this project, we tried different models to predict the probability of a passenger to survive during a shipwrek. Not surprising, we saw that women, children, and first class passengers are most likely to survive. It is a bit surprising that people with more siblings and spouses are less likely to survive. 4.2 Further Study "],
["references.html", "References", " References CruiserMapper, Cruise Ship Passenger Capacity↩ Wikipedia, List of largest passenger ships↩ "]
]
